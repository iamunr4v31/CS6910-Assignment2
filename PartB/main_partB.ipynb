{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import wandb\n",
    "\n",
    "from typing import Any, Literal\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.path.realpath('__dir__'))\n",
    "path = os.path.dirname(path)\n",
    "train_path = os.path.join(path, 'inaturalist_12K/train/')\n",
    "test_path = os.path.join(path, 'inaturalist_12K/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(augmentation: bool=True):\n",
    "    augs = [\n",
    "            transforms.RandomResizedCrop((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            # transforms.RandomRotation(45),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "    ]\n",
    "    transform = transforms.Compose([\n",
    "        *random.sample(augs, k=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
    "    ] if augmentation else [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
    "    ])\n",
    "    global train_path\n",
    "    global test_path\n",
    "    dataset = ImageFolder(train_path, transform=transform)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = (len(dataset) - train_size) \n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    test_dataset = ImageFolder(test_path, transform=transform)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_datasets()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneModel(pl.LightningModule):\n",
    "    def __init__(self, lr: float=1e-4) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        self.backbone = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_extractor = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        self.classifier = nn.Linear(self.backbone.fc.in_features, 10)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = get_datasets()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        accuracy = self.accuracy(y_hat, y)\n",
    "        self.log('train/loss', loss)\n",
    "        self.log('train/accuracy', accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        accuracy = self.accuracy(y_hat, y)\n",
    "        self.log('val/loss', loss)\n",
    "        self.log('val/accuracy', accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        accuracy = self.accuracy(y_hat, y)\n",
    "        self.log('test/loss', loss)\n",
    "        self.log('test/accuracy', accuracy)\n",
    "        return {'loss': loss, 'y_hat': y_hat, 'y': y}\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, batch_size=32, num_workers=4)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.val_dataset, batch_size=32, num_workers=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FineTuneModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miamunr4v31\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20230412_112942-s12m8noh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iamunr4v31/CS6910-Assignment2/runs/s12m8noh' target=\"_blank\">pretrained-resnet50</a></strong> to <a href='https://wandb.ai/iamunr4v31/CS6910-Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iamunr4v31/CS6910-Assignment2' target=\"_blank\">https://wandb.ai/iamunr4v31/CS6910-Assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iamunr4v31/CS6910-Assignment2/runs/s12m8noh' target=\"_blank\">https://wandb.ai/iamunr4v31/CS6910-Assignment2/runs/s12m8noh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone          | ResNet             | 25.6 M\n",
      "1 | feature_extractor | Sequential         | 23.5 M\n",
      "2 | classifier        | Linear             | 20.5 K\n",
      "3 | loss_fn           | CrossEntropyLoss   | 0     \n",
      "4 | accuracy          | MulticlassAccuracy | 0     \n",
      "---------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "25.6 M    Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.310   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 250/250 [03:32<00:00,  1.18it/s, v_num=8noh]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 250/250 [03:35<00:00,  1.16it/s, v_num=8noh]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 63/63 [00:32<00:00,  1.91it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FineTuneModel.on_test_epoch_end() missing 1 required positional argument: 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, devices\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, logger\u001b[39m=\u001b[39mlogger, callbacks\u001b[39m=\u001b[39mcallbacks, precision\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m16-mixed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m trainer\u001b[39m.\u001b[39mfit(model)\n\u001b[1;32m----> 6\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(model)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    704\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    705\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 706\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    707\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[0;32m    708\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[0;32m    746\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    747\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    748\u001b[0m )\n\u001b[1;32m--> 749\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    750\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    751\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:971\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mrun-stage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    970\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:174\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    173\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:122\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_dataloader_outputs()\n\u001b[1;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:244\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39m_evaluation_epoch_end()\n\u001b[0;32m    243\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_evaluation_epoch_end()\n\u001b[0;32m    246\u001b[0m logged_outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logged_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logged_outputs, []  \u001b[39m# free memory\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:326\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_epoch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[1;32m--> 326\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(trainer, hook_name)\n\u001b[0;32m    328\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "\u001b[1;31mTypeError\u001b[0m: FineTuneModel.on_test_epoch_end() missing 1 required positional argument: 'outputs'"
     ]
    }
   ],
   "source": [
    "logger = pl.loggers.WandbLogger(name=\"pretrained-resnet50\", project='CS6910-Assignment2')\n",
    "logger.watch(model)\n",
    "callbacks = [pl.callbacks.ModelCheckpoint(monitor='val/accuracy', mode='max', save_top_k=1, save_last=True)]\n",
    "trainer = pl.Trainer(max_epochs=20, devices=-1, logger=logger, callbacks=callbacks, precision='16-mixed')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 63/63 [06:07<00:00,  5.84s/it]\n",
      "Testing: 63it [00:34,  1.81it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FineTuneModel.on_test_epoch_end() missing 1 required positional argument: 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(model)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    704\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    705\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 706\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    707\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[0;32m    708\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[0;32m    746\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    747\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    748\u001b[0m )\n\u001b[1;32m--> 749\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    750\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    751\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:971\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mrun-stage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    970\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[1;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:174\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    173\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:122\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_dataloader_outputs()\n\u001b[1;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:244\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39m_evaluation_epoch_end()\n\u001b[0;32m    243\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_evaluation_epoch_end()\n\u001b[0;32m    246\u001b[0m logged_outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logged_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logged_outputs, []  \u001b[39m# free memory\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:326\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_epoch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_epoch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[1;32m--> 326\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(trainer, hook_name)\n\u001b[0;32m    328\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\miniconda3\\envs\\dl_class\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "\u001b[1;31mTypeError\u001b[0m: FineTuneModel.on_test_epoch_end() missing 1 required positional argument: 'outputs'"
     ]
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
